{% set name = "pydrift" %}
{% set version = 0.2.7 %}


package:
  name: {{ name|lower }}
  version: {{ version }}

source:
  url: https://pypi.io/packages/source/{{ name[0] }}/{{ name }}/{{ name }}-{{ version }}.tar.gz
  sha256: 5a8f0b1f0aca9ff341daeb8a5dab2c62d1a51d8961f3ae7ba87e09dd09069a98

build:
  number: 0
  skip: true   # [py>=40 or py2k]
  script: {{ PYTHON }} -m pip install . -vv

requirements:
  host:
    - pip
    - python
  run:
    - catboost >=0.23,<0.24
    - pandas >=1.0.3,<2.0.0
    - python
    - scikit-learn >=0.23.1,<0.24.0
    - shap >=0.35.0,<0.36.0
    - statsmodels >=0.11.1,<0.12.0
    - typing-extensions >=3.7.4,<4.0.0  # [py<38]

test:
  imports:
    - pydrift
    - pydrift.test
  commands:
    - pip check
  requires:
    - pip

about:
  home: https://github.com/sergiocalde94/pydrift
  summary: How do we measure the degradation of a machine learning process? Why does the performance of our predictive models decrease? Maybe it is that a data source has changed (one or more variables) or maybe what changes is the relationship of these variables with the target we want to predict. `pydrift` tries to facilitate this task to the data scientist, performing this kind of checks and somehow measuring that degradation.
  license: MIT
  license_file: LICENSE

extra:
  recipe-maintainers:
    - sergiocalde94
