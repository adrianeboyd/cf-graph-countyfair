{
 "PRed":[
  {
   "PR":{
    "__lazy_json__":"pr_json/196873328.json"
   },
   "data":{
    "bot_rerun":false,
    "migrator_name":"Compiler",
    "migrator_version":0
   },
   "keys":[
    "bot_rerun",
    "migrator_name",
    "migrator_version"
   ]
  },
  {
   "PR":{
    "__lazy_json__":"pr_json/215137737.json"
   },
   "data":{
    "bot_rerun":false,
    "migrator_name":"CompilerRebuild",
    "migrator_version":1,
    "name":"Python 3.7, GCC 7, R 3.5.1, openBLAS 0.3.2"
   },
   "keys":[
    "bot_rerun",
    "migrator_name",
    "migrator_version",
    "name"
   ]
  }
 ],
 "bad":{
  "code":404,
  "exception":"HTTP Error 404: Not Found",
  "traceback":[
   "Traceback (most recent call last):",
   "  File \"/root/repo/cf-scripts/conda_forge_tick/auto_tick.xsh\", line 748, in main",
   "    migrator_uid, pr_json = run(attrs=attrs, migrator=migrator, gh=gh,",
   "  File \"/root/repo/cf-scripts/conda_forge_tick/auto_tick.xsh\", line 98, in run",
   "    migrate_return = migrator.migrate(recipe_dir, attrs, **kwargs)",
   "  File \"/root/repo/cf-scripts/conda_forge_tick/migrators.xsh\", line 473, in migrate",
   "    new_patterns = self.get_hash_patterns('meta.yaml', urls, hash_type)",
   "  File \"/root/repo/cf-scripts/conda_forge_tick/migrators.xsh\", line 389, in get_hash_patterns",
   "    hash = hash_url(url, hash_type)",
   "  File \"/opt/conda/envs/run_env/lib/python3.8/site-packages/rever/tools.xsh\", line 207, in hash_url",
   "    for b in stream_url_progress(url, verb='Hashing', quiet=quiet):",
   "  File \"/opt/conda/envs/run_env/lib/python3.8/site-packages/rever/tools.xsh\", line 179, in stream_url_progress",
   "    with urllib.request.urlopen(url) as f:",
   "  File \"/opt/conda/envs/run_env/lib/python3.8/urllib/request.py\", line 222, in urlopen",
   "    return opener.open(url, data, timeout)",
   "  File \"/opt/conda/envs/run_env/lib/python3.8/urllib/request.py\", line 531, in open",
   "    response = meth(req, response)",
   "  File \"/opt/conda/envs/run_env/lib/python3.8/urllib/request.py\", line 640, in http_response",
   "    response = self.parent.error(",
   "  File \"/opt/conda/envs/run_env/lib/python3.8/urllib/request.py\", line 563, in error",
   "    result = self._call_chain(*args)",
   "  File \"/opt/conda/envs/run_env/lib/python3.8/urllib/request.py\", line 502, in _call_chain",
   "    result = func(*args)",
   "  File \"/opt/conda/envs/run_env/lib/python3.8/urllib/request.py\", line 755, in http_error_302",
   "    return self.parent.open(new, timeout=req.timeout)",
   "  File \"/opt/conda/envs/run_env/lib/python3.8/urllib/request.py\", line 531, in open",
   "    response = meth(req, response)",
   "  File \"/opt/conda/envs/run_env/lib/python3.8/urllib/request.py\", line 640, in http_response",
   "    response = self.parent.error(",
   "  File \"/opt/conda/envs/run_env/lib/python3.8/urllib/request.py\", line 569, in error",
   "    return self._call_chain(*args)",
   "  File \"/opt/conda/envs/run_env/lib/python3.8/urllib/request.py\", line 502, in _call_chain",
   "    result = func(*args)",
   "  File \"/opt/conda/envs/run_env/lib/python3.8/urllib/request.py\", line 649, in http_error_default",
   "    raise HTTPError(req.full_url, code, msg, hdrs, fp)",
   "urllib.error.HTTPError: HTTP Error 404: Not Found",
   ""
  ],
  "url":"https://codeload.github.com/adrianlopezroche/fdupes/tar.gz/v1.51"
 },
 "conda-forge.yml":{
  "compiler_stack":"comp7",
  "max_py_ver":"37",
  "max_r_ver":"35"
 },
 "feedstock_name":"fdupes",
 "hash_type":"sha256",
 "meta_yaml":{
  "about":{
   "description":"FDUPES is a program for identifying or deleting duplicate files residing\nwithin specified directories.\n",
   "dev_url":"https://github.com/adrianlopezroche/fdupes",
   "home":"https://github.com/adrianlopezroche/fdupes",
   "license":"MIT",
   "license_family":"MIT",
   "license_file":"README",
   "summary":"Identify or delete duplicate files"
  },
  "build":{
   "number":"1000",
   "skip":true
  },
  "extra":{
   "recipe-maintainers":[
    "keuv-grvl",
    "keuv-grvl",
    "keuv-grvl"
   ]
  },
  "package":{
   "name":"fdupes",
   "version":"1.6.1"
  },
  "requirements":{
   "build":[
    "c_compiler_stub",
    "c_compiler_stub",
    "c_compiler_stub"
   ]
  },
  "source":{
   "fn":"fdupes-1.6.1.tar.gz",
   "sha256":"9d6b6fdb0b8419815b4df3bdfd0aebc135b8276c90bbbe78ebe6af0b88ba49ea",
   "url":"https://github.com/adrianlopezroche/fdupes/archive/v1.6.1.tar.gz"
  },
  "test":{
   "commands":[
    "fdupes --version",
    "fdupes --version",
    "fdupes --version"
   ]
  }
 },
 "name":"fdupes",
 "new_version":"1.51",
 "pinning_version":"2018.09.11",
 "raw_meta_yaml":"\n{% set name = \"fdupes\" %}\n{% set version = \"1.6.1\" %}\n{% set sha256 = \"9d6b6fdb0b8419815b4df3bdfd0aebc135b8276c90bbbe78ebe6af0b88ba49ea\" %}\n\npackage:\n  name: {{ name|lower }}\n  version: {{ version }}\n\nsource:\n  fn: {{ name }}-{{ version }}.tar.gz\n  url: https://github.com/adrianlopezroche/{{ name }}/archive/v{{ version }}.tar.gz\n  sha256: {{ sha256 }}\n\nbuild:\n  number: 1000\n  skip: True  # [win]\n\nrequirements:\n  build:\n    - {{ compiler('c') }}\n\ntest:\n  commands:\n    - fdupes --version\n\nabout:\n  home: https://github.com/adrianlopezroche/fdupes\n  license: MIT\n  license_family: MIT\n  license_file: README\n  summary: Identify or delete duplicate files\n  description : |\n    FDUPES is a program for identifying or deleting duplicate files residing\n    within specified directories.\n  dev_url: https://github.com/adrianlopezroche/fdupes\n\nextra:\n  recipe-maintainers:\n    - keuv-grvl\n",
 "req":{
  "__set__":true,
  "elements":[
   "c_compiler_stub"
  ]
 },
 "smithy_version":"3.1.12",
 "url":"https://github.com/adrianlopezroche/fdupes/archive/v1.6.1.tar.gz",
 "version":"1.6.1"
}