{
 "bad":{
  "code":404,
  "exception":"HTTP Error 404: Not Found",
  "traceback":[
   "Traceback (most recent call last):",
   "  File \"/root/repo/cf-scripts/conda_forge_tick/auto_tick.xsh\", line 745, in main",
   "    hash_type=attrs.get('hash_type', 'sha256'))",
   "  File \"/root/repo/cf-scripts/conda_forge_tick/auto_tick.xsh\", line 98, in run",
   "    migrate_return = migrator.migrate(recipe_dir, attrs, **kwargs)",
   "  File \"/root/repo/cf-scripts/conda_forge_tick/migrators.xsh\", line 467, in migrate",
   "    new_patterns = self.get_hash_patterns('meta.yaml', urls, hash_type)",
   "  File \"/root/repo/cf-scripts/conda_forge_tick/migrators.xsh\", line 384, in get_hash_patterns",
   "    hash = hash_url(url, hash_type)",
   "  File \"/opt/conda/envs/run_env/lib/python3.7/site-packages/rever/tools.xsh\", line 207, in hash_url",
   "    for b in stream_url_progress(url, verb='Hashing', quiet=quiet):",
   "  File \"/opt/conda/envs/run_env/lib/python3.7/site-packages/rever/tools.xsh\", line 179, in stream_url_progress",
   "    with urllib.request.urlopen(url) as f:",
   "  File \"/opt/conda/envs/run_env/lib/python3.7/urllib/request.py\", line 222, in urlopen",
   "    return opener.open(url, data, timeout)",
   "  File \"/opt/conda/envs/run_env/lib/python3.7/urllib/request.py\", line 531, in open",
   "    response = meth(req, response)",
   "  File \"/opt/conda/envs/run_env/lib/python3.7/urllib/request.py\", line 641, in http_response",
   "    'http', request, response, code, msg, hdrs)",
   "  File \"/opt/conda/envs/run_env/lib/python3.7/urllib/request.py\", line 563, in error",
   "    result = self._call_chain(*args)",
   "  File \"/opt/conda/envs/run_env/lib/python3.7/urllib/request.py\", line 503, in _call_chain",
   "    result = func(*args)",
   "  File \"/opt/conda/envs/run_env/lib/python3.7/urllib/request.py\", line 755, in http_error_302",
   "    return self.parent.open(new, timeout=req.timeout)",
   "  File \"/opt/conda/envs/run_env/lib/python3.7/urllib/request.py\", line 531, in open",
   "    response = meth(req, response)",
   "  File \"/opt/conda/envs/run_env/lib/python3.7/urllib/request.py\", line 641, in http_response",
   "    'http', request, response, code, msg, hdrs)",
   "  File \"/opt/conda/envs/run_env/lib/python3.7/urllib/request.py\", line 569, in error",
   "    return self._call_chain(*args)",
   "  File \"/opt/conda/envs/run_env/lib/python3.7/urllib/request.py\", line 503, in _call_chain",
   "    result = func(*args)",
   "  File \"/opt/conda/envs/run_env/lib/python3.7/urllib/request.py\", line 649, in http_error_default",
   "    raise HTTPError(req.full_url, code, msg, hdrs, fp)",
   "urllib.error.HTTPError: HTTP Error 404: Not Found",
   ""
  ],
  "url":"https://codeload.github.com/NVIDIA/nccl/zip/v2.5.6-2-1"
 },
 "conda-forge.yml":{},
 "feedstock_name":"nccl",
 "hash_type":"sha256",
 "meta_yaml":{
  "about":{
   "description":"The NVIDIA Collective Communications Library (NCCL) implements multi-GPU\nand multi-node collective communication primitives that are performance\noptimized for NVIDIA GPUs. NCCL provides routines such as all-gather,\nall-reduce, broadcast, reduce, reduce-scatter, that are optimized to\nachieve high bandwidth over PCIe and NVLink high-speed interconnect.\n",
   "dev_url":"https://github.com/NVIDIA/nccl",
   "doc_url":"https://docs.nvidia.com/deeplearning/sdk/nccl-developer-guide/docs/index.html",
   "home":"https://developer.nvidia.com/nccl",
   "license":"BSD-3-Clause",
   "license_family":"BSD",
   "license_file":"LICENSE.txt",
   "summary":"Optimized primitives for collective multi-GPU communication"
  },
  "build":{
   "number":"0",
   "run_exports":[
    "subpackage_stub",
    "subpackage_stub",
    "subpackage_stub"
   ],
   "skip":true
  },
  "extra":{
   "recipe-maintainers":[
    "jakirkham",
    "jakirkham",
    "jakirkham"
   ]
  },
  "package":{
   "name":"nccl",
   "version":"2.5.6.1"
  },
  "requirements":{
   "build":[
    "c_compiler_stub",
    "cxx_compiler_stub",
    "cuda_compiler_stub",
    "make",
    "c_compiler_stub",
    "cxx_compiler_stub",
    "cuda_compiler_stub",
    "make",
    "c_compiler_stub",
    "cxx_compiler_stub",
    "cuda_compiler_stub",
    "make"
   ]
  },
  "source":{
   "sha256":"38a37d98be11f43232b988719226866b407f08b9666dcaf345796bd8f354ef54",
   "url":"https://github.com/NVIDIA/nccl/archive/v2.5.6-1.tar.gz"
  },
  "test":{
   "commands":[
    "test -f \"${PREFIX}/include/nccl.h\"",
    "test -f \"${PREFIX}/lib/libnccl.so\"",
    "test -f \"${PREFIX}/lib/libnccl_static.a\"",
    "test -f \"${PREFIX}/include/nccl.h\"",
    "test -f \"${PREFIX}/lib/libnccl.so\"",
    "test -f \"${PREFIX}/lib/libnccl_static.a\"",
    "test -f \"${PREFIX}/include/nccl.h\"",
    "test -f \"${PREFIX}/lib/libnccl.so\"",
    "test -f \"${PREFIX}/lib/libnccl_static.a\""
   ]
  }
 },
 "name":"nccl",
 "new_version":"2.5.6-2",
 "raw_meta_yaml":"{% set name = \"nccl\" %}\n{% set version = \"2.5.6\" %}\n{% set revision = \"1\" %}\n\npackage:\n  name: {{ name|lower }}\n  version: {{ version }}.{{ revision }}\n\nsource:\n  url: https://github.com/NVIDIA/nccl/archive/v{{ version }}-{{ revision }}.tar.gz\n  sha256: 38a37d98be11f43232b988719226866b407f08b9666dcaf345796bd8f354ef54\n\nbuild:\n  number: 0\n  skip: true  # [(not linux64) or (cuda_compiler_version == \"None\")]\n  run_exports:\n    # xref: https://github.com/NVIDIA/nccl/issues/218\n    - {{ pin_subpackage(name, max_pin=\"x\") }}\n\nrequirements:\n  build:\n    - {{ compiler(\"c\") }}\n    - {{ compiler(\"cxx\") }}\n    - {{ compiler(\"cuda\") }}\n    - make\n\ntest:\n  commands:\n    - test -f \"${PREFIX}/include/nccl.h\"\n    - test -f \"${PREFIX}/lib/libnccl.so\"\n    - test -f \"${PREFIX}/lib/libnccl_static.a\"\n\nabout:\n  home: https://developer.nvidia.com/nccl\n  license: BSD-3-Clause\n  license_family: BSD\n  license_file: LICENSE.txt\n  summary: Optimized primitives for collective multi-GPU communication\n\n  description: |\n    The NVIDIA Collective Communications Library (NCCL) implements multi-GPU\n    and multi-node collective communication primitives that are performance\n    optimized for NVIDIA GPUs. NCCL provides routines such as all-gather,\n    all-reduce, broadcast, reduce, reduce-scatter, that are optimized to\n    achieve high bandwidth over PCIe and NVLink high-speed interconnect.\n\n  doc_url: https://docs.nvidia.com/deeplearning/sdk/nccl-developer-guide/docs/index.html\n  dev_url: https://github.com/NVIDIA/nccl\n\nextra:\n  recipe-maintainers:\n    - jakirkham\n",
 "req":{
  "__set__":true,
  "elements":[
   "c_compiler_stub",
   "cuda_compiler_stub",
   "cxx_compiler_stub",
   "make"
  ]
 },
 "url":"https://github.com/NVIDIA/nccl/archive/v2.4.6-1.tar.gz",
 "version":"2.5.6.1"
}