{
 "PRed":[
  {
   "PR":{
    "__lazy_json__":"pr_json/447105311.json"
   },
   "data":{
    "bot_rerun":false,
    "migrator_name":"Version",
    "migrator_version":0,
    "version":"0.1.92"
   },
   "keys":[
    "bot_rerun",
    "migrator_name",
    "migrator_version",
    "version"
   ]
  },
  {
   "PR":{
    "__lazy_json__":"pr_json/447106698.json"
   },
   "data":{
    "bot_rerun":false,
    "migrator_name":"MigrationYaml",
    "migrator_object_version":1,
    "migrator_version":0,
    "name":"python38"
   },
   "keys":[
    "bot_rerun",
    "migrator_name",
    "migrator_object_version",
    "migrator_version",
    "name"
   ]
  },
  {
   "PR":{
    "__lazy_json__":"pr_json/447108933.json"
   },
   "data":{
    "bot_rerun":false,
    "migrator_name":"MigrationYaml",
    "migrator_object_version":1,
    "migrator_version":0,
    "name":"pypy"
   },
   "keys":[
    "bot_rerun",
    "migrator_name",
    "migrator_object_version",
    "migrator_version",
    "name"
   ]
  },
  {
   "PR":{
    "__lazy_json__":"pr_json/453211476.json"
   },
   "data":{
    "bot_rerun":false,
    "migrator_name":"Version",
    "migrator_version":0,
    "version":"1.0.0"
   },
   "keys":[
    "bot_rerun",
    "migrator_name",
    "migrator_version",
    "version"
   ]
  }
 ],
 "archived":false,
 "bad":false,
 "conda-forge.yml":{},
 "feedstock_name":"sentencepiece",
 "hash_type":"sha256",
 "linux_64_meta_yaml":{
  "about":{
   "home":"https://github.com/google/sentencepiece/",
   "license":"Apache-2.0",
   "license_family":"Apache",
   "license_file":"LICENSE",
   "summary":"SentencePiece is an unsupervised text tokenizer and detokenizer mainly for Neural Network-based text generation systems where the vocabulary size is predetermined prior to the neural model training."
  },
  "build":{
   "number":"1"
  },
  "extra":{
   "recipe-maintainers":[
    "setu4993"
   ]
  },
  "package":{
   "name":"sentencepiece",
   "version":"0.1.91"
  },
  "requirements":{
   "build":[
    "cmake",
    "cxx_compiler_stub",
    "pkg-config",
    "gperftools",
    "make"
   ],
   "host":[
    "pip",
    "python",
    "gperftools"
   ],
   "run":[
    "python",
    "gperftools"
   ]
  },
  "source":{
   "sha256":"acbc7ea12713cd2a8d64892f8d2033c7fd2bb4faecab39452496120ace9a4b1b",
   "url":"https://github.com/google/sentencepiece/archive/v0.1.91.tar.gz"
  },
  "test":{
   "commands":[
    "spm_export_vocab --help",
    "spm_normalize --help"
   ],
   "imports":[
    "sentencepiece"
   ]
  }
 },
 "linux_64_requirements":{
  "build":{
   "__set__":true,
   "elements":[
    "cmake",
    "cxx_compiler_stub",
    "gperftools",
    "make",
    "pkg-config"
   ]
  },
  "host":{
   "__set__":true,
   "elements":[
    "gperftools",
    "pip",
    "python"
   ]
  },
  "run":{
   "__set__":true,
   "elements":[
    "gperftools",
    "python"
   ]
  },
  "test":{
   "__set__":true,
   "elements":[]
  }
 },
 "meta_yaml":{
  "about":{
   "home":"https://github.com/google/sentencepiece/",
   "license":"Apache-2.0",
   "license_family":"Apache",
   "license_file":"LICENSE",
   "summary":"SentencePiece is an unsupervised text tokenizer and detokenizer mainly for Neural Network-based text generation systems where the vocabulary size is predetermined prior to the neural model training."
  },
  "build":{
   "number":"1",
   "skip":true
  },
  "extra":{
   "recipe-maintainers":[
    "setu4993",
    "setu4993",
    "setu4993"
   ]
  },
  "package":{
   "name":"sentencepiece",
   "version":"0.1.91"
  },
  "requirements":{
   "build":[
    "cmake",
    "cxx_compiler_stub",
    "pkg-config",
    "gperftools",
    "make",
    "cmake",
    "cxx_compiler_stub",
    "pkg-config",
    "gperftools",
    "make",
    "cmake",
    "cxx_compiler_stub",
    "pkg-config",
    "gperftools",
    "make"
   ],
   "host":[
    "pip",
    "python",
    "gperftools",
    "pip",
    "python",
    "gperftools",
    "pip",
    "python",
    "gperftools"
   ],
   "run":[
    "python",
    "gperftools",
    "python",
    "gperftools",
    "python",
    "gperftools"
   ]
  },
  "source":{
   "sha256":"acbc7ea12713cd2a8d64892f8d2033c7fd2bb4faecab39452496120ace9a4b1b",
   "url":"https://github.com/google/sentencepiece/archive/v0.1.91.tar.gz"
  },
  "test":{
   "commands":[
    "spm_export_vocab --help",
    "spm_normalize --help",
    "spm_export_vocab --help",
    "spm_normalize --help",
    "spm_export_vocab --help",
    "spm_normalize --help"
   ],
   "imports":[
    "sentencepiece",
    "sentencepiece",
    "sentencepiece"
   ]
  }
 },
 "name":"sentencepiece",
 "new_version":"0.1.92",
 "new_version_attempts":{
  "0.1.92":1,
  "1.0.0":1
 },
 "new_version_errors":{},
 "osx_64_meta_yaml":{
  "about":{
   "home":"https://github.com/google/sentencepiece/",
   "license":"Apache-2.0",
   "license_family":"Apache",
   "license_file":"LICENSE",
   "summary":"SentencePiece is an unsupervised text tokenizer and detokenizer mainly for Neural Network-based text generation systems where the vocabulary size is predetermined prior to the neural model training."
  },
  "build":{
   "number":"1",
   "skip":true
  },
  "extra":{
   "recipe-maintainers":[
    "setu4993"
   ]
  },
  "package":{
   "name":"sentencepiece",
   "version":"0.1.91"
  },
  "requirements":{
   "build":[
    "cmake",
    "cxx_compiler_stub",
    "pkg-config",
    "gperftools",
    "make"
   ],
   "host":[
    "pip",
    "python",
    "gperftools"
   ],
   "run":[
    "python",
    "gperftools"
   ]
  },
  "source":{
   "sha256":"acbc7ea12713cd2a8d64892f8d2033c7fd2bb4faecab39452496120ace9a4b1b",
   "url":"https://github.com/google/sentencepiece/archive/v0.1.91.tar.gz"
  },
  "test":{
   "commands":[
    "spm_export_vocab --help",
    "spm_normalize --help"
   ],
   "imports":[
    "sentencepiece"
   ]
  }
 },
 "osx_64_requirements":{
  "build":{
   "__set__":true,
   "elements":[
    "cmake",
    "cxx_compiler_stub",
    "gperftools",
    "make",
    "pkg-config"
   ]
  },
  "host":{
   "__set__":true,
   "elements":[
    "gperftools",
    "pip",
    "python"
   ]
  },
  "run":{
   "__set__":true,
   "elements":[
    "gperftools",
    "python"
   ]
  },
  "test":{
   "__set__":true,
   "elements":[]
  }
 },
 "pinning_version":"2020.07.19.19.30.49",
 "raw_meta_yaml":"{% set name = \"sentencepiece\" %}\n{% set version = \"0.1.91\" %}\n\npackage:\n  name: \"{{ name|lower }}\"\n  version: \"{{ version }}\"\n\nsource:\n  url: https://github.com/google/{{ name }}/archive/v{{ version }}.tar.gz\n  sha256: acbc7ea12713cd2a8d64892f8d2033c7fd2bb4faecab39452496120ace9a4b1b\n\nbuild:\n  number: 1\n  skip: True  # [py<36 or win or osx]\n\nrequirements:\n  build:\n    - cmake\n    - {{ compiler('cxx') }}\n    - pkg-config\n    - gperftools\n    - make\n  host:\n    - pip\n    - python\n    - gperftools\n  run:\n    - python\n    - gperftools\n\ntest:\n  imports:\n    - sentencepiece\n  commands:\n    - spm_export_vocab --help\n    - spm_normalize --help\n\nabout:\n  home: \"https://github.com/google/sentencepiece/\"\n  license: Apache-2.0\n  license_family: Apache\n  license_file: LICENSE\n  summary: \"SentencePiece is an unsupervised text tokenizer and detokenizer mainly for Neural Network-based text generation systems where the vocabulary size is predetermined prior to the neural model training.\"\n\nextra:\n  recipe-maintainers:\n    - setu4993\n",
 "req":{
  "__set__":true,
  "elements":[
   "cmake",
   "cxx_compiler_stub",
   "gperftools",
   "make",
   "pip",
   "pkg-config",
   "python"
  ]
 },
 "requirements":{
  "build":{
   "__set__":true,
   "elements":[
    "cmake",
    "cxx_compiler_stub",
    "gperftools",
    "make",
    "pkg-config"
   ]
  },
  "host":{
   "__set__":true,
   "elements":[
    "gperftools",
    "pip",
    "python"
   ]
  },
  "run":{
   "__set__":true,
   "elements":[
    "gperftools",
    "python"
   ]
  },
  "test":{
   "__set__":true,
   "elements":[]
  }
 },
 "smithy_version":"No azure token. Create a token and\nput it in ~/.conda-smithy/azure.token\n3.7.4\n",
 "strong_exports":false,
 "total_requirements":{
  "build":{
   "__set__":true,
   "elements":[
    "cmake",
    "cxx_compiler_stub",
    "gperftools",
    "make",
    "pkg-config"
   ]
  },
  "host":{
   "__set__":true,
   "elements":[
    "gperftools",
    "pip",
    "python"
   ]
  },
  "run":{
   "__set__":true,
   "elements":[
    "gperftools",
    "python"
   ]
  },
  "test":{
   "__set__":true,
   "elements":[]
  }
 },
 "url":"https://github.com/google/sentencepiece/archive/v0.1.91.tar.gz",
 "version":"0.1.91",
 "win_64_meta_yaml":{
  "about":{
   "home":"https://github.com/google/sentencepiece/",
   "license":"Apache-2.0",
   "license_family":"Apache",
   "license_file":"LICENSE",
   "summary":"SentencePiece is an unsupervised text tokenizer and detokenizer mainly for Neural Network-based text generation systems where the vocabulary size is predetermined prior to the neural model training."
  },
  "build":{
   "number":"1",
   "skip":true
  },
  "extra":{
   "recipe-maintainers":[
    "setu4993"
   ]
  },
  "package":{
   "name":"sentencepiece",
   "version":"0.1.91"
  },
  "requirements":{
   "build":[
    "cmake",
    "cxx_compiler_stub",
    "pkg-config",
    "gperftools",
    "make"
   ],
   "host":[
    "pip",
    "python",
    "gperftools"
   ],
   "run":[
    "python",
    "gperftools"
   ]
  },
  "source":{
   "sha256":"acbc7ea12713cd2a8d64892f8d2033c7fd2bb4faecab39452496120ace9a4b1b",
   "url":"https://github.com/google/sentencepiece/archive/v0.1.91.tar.gz"
  },
  "test":{
   "commands":[
    "spm_export_vocab --help",
    "spm_normalize --help"
   ],
   "imports":[
    "sentencepiece"
   ]
  }
 },
 "win_64_requirements":{
  "build":{
   "__set__":true,
   "elements":[
    "cmake",
    "cxx_compiler_stub",
    "gperftools",
    "make",
    "pkg-config"
   ]
  },
  "host":{
   "__set__":true,
   "elements":[
    "gperftools",
    "pip",
    "python"
   ]
  },
  "run":{
   "__set__":true,
   "elements":[
    "gperftools",
    "python"
   ]
  },
  "test":{
   "__set__":true,
   "elements":[]
  }
 }
}