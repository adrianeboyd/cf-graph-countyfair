{
 "PRed":[
  {
   "PR":{
    "__lazy_json__":"pr_json/453665009.json"
   },
   "data":{
    "bot_rerun":false,
    "migrator_name":"MigrationYaml",
    "migrator_object_version":1,
    "migrator_version":0,
    "name":"pypy"
   },
   "keys":[
    "bot_rerun",
    "migrator_name",
    "migrator_object_version",
    "migrator_version",
    "name"
   ]
  },
  {
   "PR":{
    "__lazy_json__":"pr_json/12038945-1025-4d84-88aa-973298bfa2d7.json"
   },
   "data":{
    "bot_rerun":false,
    "migrator_name":"MigrationYaml",
    "migrator_object_version":1,
    "migrator_version":0,
    "name":"python38"
   },
   "keys":[
    "bot_rerun",
    "migrator_name",
    "migrator_object_version",
    "migrator_version",
    "name"
   ]
  }
 ],
 "archived":false,
 "bad":false,
 "conda-forge.yml":{},
 "feedstock_name":"tokenizers",
 "hash_type":"sha256",
 "linux_64_meta_yaml":{
  "about":{
   "home":"https://github.com/huggingface/tokenizers",
   "license":"Apache-2.0",
   "license_family":"APACHE",
   "license_file":"LICENSE",
   "summary":"Fast State-of-the-Art Tokenizers optimized for Research and Production"
  },
  "build":{
   "missing_dso_whitelist":[
    "/usr/lib64/libgcc_s.so.1"
   ],
   "number":"1",
   "script":[
    "-m pip install . --no-deps --ignore-installed -vv"
   ]
  },
  "extra":{
   "recipe-maintainers":[
    "oblute",
    "rluria14",
    "ndmaxar",
    "setu4993"
   ]
  },
  "package":{
   "name":"tokenizers",
   "version":"0.8.0"
  },
  "requirements":{
   "build":[
    "rust_compiler_stub"
   ],
   "host":[
    "python",
    "pip",
    "setuptools-rust",
    "setuptools"
   ],
   "run":[
    "python",
    "libgcc-ng"
   ]
  },
  "source":{
   "sha256":"703101ffc1cce87e39a8fa9754126a5c29590b03817a73727e3268474dc716e6",
   "url":"https://pypi.io/packages/source/t/tokenizers/tokenizers-0.8.0.tar.gz"
  },
  "test":{
   "imports":[
    "tokenizers",
    "tokenizers.models",
    "tokenizers.decoders",
    "tokenizers.normalizers",
    "tokenizers.pre_tokenizers",
    "tokenizers.processors",
    "tokenizers.trainers",
    "tokenizers.implementations"
   ]
  }
 },
 "linux_64_requirements":{
  "build":{
   "__set__":true,
   "elements":[
    "rust_compiler_stub"
   ]
  },
  "host":{
   "__set__":true,
   "elements":[
    "pip",
    "python",
    "setuptools",
    "setuptools-rust"
   ]
  },
  "run":{
   "__set__":true,
   "elements":[
    "libgcc-ng",
    "python"
   ]
  },
  "test":{
   "__set__":true,
   "elements":[]
  }
 },
 "meta_yaml":{
  "about":{
   "home":"https://github.com/huggingface/tokenizers",
   "license":"Apache-2.0",
   "license_family":"APACHE",
   "license_file":"LICENSE",
   "summary":"Fast State-of-the-Art Tokenizers optimized for Research and Production"
  },
  "build":{
   "missing_dso_whitelist":[
    "/usr/lib64/libgcc_s.so.1"
   ],
   "number":"1",
   "script":[
    "-m pip install . --no-deps --ignore-installed -vv",
    "-m pip install . --no-deps --ignore-installed -vv",
    "-m pip install . --no-deps --ignore-installed -vv"
   ],
   "skip":true
  },
  "extra":{
   "recipe-maintainers":[
    "oblute",
    "rluria14",
    "ndmaxar",
    "setu4993",
    "oblute",
    "rluria14",
    "ndmaxar",
    "setu4993",
    "oblute",
    "rluria14",
    "ndmaxar",
    "setu4993"
   ]
  },
  "package":{
   "name":"tokenizers",
   "version":"0.8.0"
  },
  "requirements":{
   "build":[
    "rust_compiler_stub",
    "rust_compiler_stub",
    "rust_compiler_stub"
   ],
   "host":[
    "python",
    "pip",
    "setuptools-rust",
    "setuptools",
    "python",
    "pip",
    "setuptools-rust",
    "setuptools",
    "python",
    "pip",
    "setuptools-rust",
    "setuptools"
   ],
   "run":[
    "python",
    "python",
    "python",
    "libgcc-ng"
   ]
  },
  "source":{
   "sha256":"703101ffc1cce87e39a8fa9754126a5c29590b03817a73727e3268474dc716e6",
   "url":"https://pypi.io/packages/source/t/tokenizers/tokenizers-0.8.0.tar.gz"
  },
  "test":{
   "imports":[
    "tokenizers",
    "tokenizers.models",
    "tokenizers.decoders",
    "tokenizers.normalizers",
    "tokenizers.pre_tokenizers",
    "tokenizers.processors",
    "tokenizers.trainers",
    "tokenizers.implementations",
    "tokenizers",
    "tokenizers.models",
    "tokenizers.decoders",
    "tokenizers.normalizers",
    "tokenizers.pre_tokenizers",
    "tokenizers.processors",
    "tokenizers.trainers",
    "tokenizers.implementations",
    "tokenizers",
    "tokenizers.models",
    "tokenizers.decoders",
    "tokenizers.normalizers",
    "tokenizers.pre_tokenizers",
    "tokenizers.processors",
    "tokenizers.trainers",
    "tokenizers.implementations"
   ]
  }
 },
 "name":"tokenizers",
 "new_version":"0.8.0",
 "osx_64_meta_yaml":{
  "about":{
   "home":"https://github.com/huggingface/tokenizers",
   "license":"Apache-2.0",
   "license_family":"APACHE",
   "license_file":"LICENSE",
   "summary":"Fast State-of-the-Art Tokenizers optimized for Research and Production"
  },
  "build":{
   "missing_dso_whitelist":[
    "/usr/lib/libresolv.9.dylib"
   ],
   "number":"1",
   "script":[
    "-m pip install . --no-deps --ignore-installed -vv"
   ]
  },
  "extra":{
   "recipe-maintainers":[
    "oblute",
    "rluria14",
    "ndmaxar",
    "setu4993"
   ]
  },
  "package":{
   "name":"tokenizers",
   "version":"0.8.0"
  },
  "requirements":{
   "build":[
    "rust_compiler_stub"
   ],
   "host":[
    "python",
    "pip",
    "setuptools-rust",
    "setuptools"
   ],
   "run":[
    "python"
   ]
  },
  "source":{
   "sha256":"703101ffc1cce87e39a8fa9754126a5c29590b03817a73727e3268474dc716e6",
   "url":"https://pypi.io/packages/source/t/tokenizers/tokenizers-0.8.0.tar.gz"
  },
  "test":{
   "imports":[
    "tokenizers",
    "tokenizers.models",
    "tokenizers.decoders",
    "tokenizers.normalizers",
    "tokenizers.pre_tokenizers",
    "tokenizers.processors",
    "tokenizers.trainers",
    "tokenizers.implementations"
   ]
  }
 },
 "osx_64_requirements":{
  "build":{
   "__set__":true,
   "elements":[
    "rust_compiler_stub"
   ]
  },
  "host":{
   "__set__":true,
   "elements":[
    "pip",
    "python",
    "setuptools",
    "setuptools-rust"
   ]
  },
  "run":{
   "__set__":true,
   "elements":[
    "python"
   ]
  },
  "test":{
   "__set__":true,
   "elements":[]
  }
 },
 "pinning_version":"2020.07.19.19.30.49",
 "raw_meta_yaml":"{% set name = \"tokenizers\" %}\n{% set version = \"0.8.0\" %}\n\npackage:\n  name: {{ name|lower }}\n  version: {{ version }}\n\nsource:\n  url: https://pypi.io/packages/source/{{ name[0] }}/{{ name }}/{{ name }}-{{ version }}.tar.gz\n  sha256: 703101ffc1cce87e39a8fa9754126a5c29590b03817a73727e3268474dc716e6\n\nbuild:\n  number: 1\n  script:\n    - {{ PYTHON }} -m pip install . --no-deps --ignore-installed -vv\n  missing_dso_whitelist:\n    - /usr/lib/libresolv.9.dylib  # [osx]\n    - /usr/lib64/libgcc_s.so.1  # [linux]\n  skip: True  # [win]\n\nrequirements:\n  build:\n    - {{ compiler('rust') }}\n  host:\n    - python\n    - pip\n    - setuptools-rust\n    - setuptools\n  run:\n    - python\n    - libgcc-ng                                                  # [linux]\n\ntest:\n  imports:\n    - tokenizers\n    - tokenizers.models\n    - tokenizers.decoders\n    - tokenizers.normalizers\n    - tokenizers.pre_tokenizers\n    - tokenizers.processors\n    - tokenizers.trainers\n    - tokenizers.implementations\n\nabout:\n  home: https://github.com/huggingface/tokenizers\n  license: Apache-2.0\n  license_family: APACHE\n  license_file: LICENSE\n  summary: \"Fast State-of-the-Art Tokenizers optimized for Research and Production\"\n\nextra:\n  recipe-maintainers:\n    - oblute\n    - rluria14\n    - ndmaxar\n    - setu4993\n",
 "req":{
  "__set__":true,
  "elements":[
   "libgcc-ng",
   "pip",
   "python",
   "rust_compiler_stub",
   "setuptools",
   "setuptools-rust"
  ]
 },
 "requirements":{
  "build":{
   "__set__":true,
   "elements":[
    "rust_compiler_stub"
   ]
  },
  "host":{
   "__set__":true,
   "elements":[
    "pip",
    "python",
    "setuptools",
    "setuptools-rust"
   ]
  },
  "run":{
   "__set__":true,
   "elements":[
    "libgcc-ng",
    "python"
   ]
  },
  "test":{
   "__set__":true,
   "elements":[]
  }
 },
 "smithy_version":"No azure token. Create a token and\nput it in ~/.conda-smithy/azure.token\n3.7.4\n",
 "strong_exports":false,
 "total_requirements":{
  "build":{
   "__set__":true,
   "elements":[
    "rust_compiler_stub"
   ]
  },
  "host":{
   "__set__":true,
   "elements":[
    "pip",
    "python",
    "setuptools",
    "setuptools-rust"
   ]
  },
  "run":{
   "__set__":true,
   "elements":[
    "libgcc-ng",
    "python"
   ]
  },
  "test":{
   "__set__":true,
   "elements":[]
  }
 },
 "url":"https://pypi.io/packages/source/t/tokenizers/tokenizers-0.8.0.tar.gz",
 "version":"0.8.0",
 "win_64_meta_yaml":{
  "about":{
   "home":"https://github.com/huggingface/tokenizers",
   "license":"Apache-2.0",
   "license_family":"APACHE",
   "license_file":"LICENSE",
   "summary":"Fast State-of-the-Art Tokenizers optimized for Research and Production"
  },
  "build":{
   "missing_dso_whitelist":null,
   "number":"1",
   "script":[
    "-m pip install . --no-deps --ignore-installed -vv"
   ],
   "skip":true
  },
  "extra":{
   "recipe-maintainers":[
    "oblute",
    "rluria14",
    "ndmaxar",
    "setu4993"
   ]
  },
  "package":{
   "name":"tokenizers",
   "version":"0.8.0"
  },
  "requirements":{
   "build":[
    "rust_compiler_stub"
   ],
   "host":[
    "python",
    "pip",
    "setuptools-rust",
    "setuptools"
   ],
   "run":[
    "python"
   ]
  },
  "source":{
   "sha256":"703101ffc1cce87e39a8fa9754126a5c29590b03817a73727e3268474dc716e6",
   "url":"https://pypi.io/packages/source/t/tokenizers/tokenizers-0.8.0.tar.gz"
  },
  "test":{
   "imports":[
    "tokenizers",
    "tokenizers.models",
    "tokenizers.decoders",
    "tokenizers.normalizers",
    "tokenizers.pre_tokenizers",
    "tokenizers.processors",
    "tokenizers.trainers",
    "tokenizers.implementations"
   ]
  }
 },
 "win_64_requirements":{
  "build":{
   "__set__":true,
   "elements":[
    "rust_compiler_stub"
   ]
  },
  "host":{
   "__set__":true,
   "elements":[
    "pip",
    "python",
    "setuptools",
    "setuptools-rust"
   ]
  },
  "run":{
   "__set__":true,
   "elements":[
    "python"
   ]
  },
  "test":{
   "__set__":true,
   "elements":[]
  }
 }
}